{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dedd3b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16155\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "306514e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\16155\\.cache\\kagglehub\\datasets\\polartech\\nike-sportwear-product-dataset\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"polartech/nike-sportwear-product-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7265889d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Nike_UK_2022-09-01.csv into DataFrame.\n"
     ]
    }
   ],
   "source": [
    "# Load dataset file(s) into a pandas DataFrame\n",
    "# Assuming there's a CSV file in the folder\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith('.csv'):\n",
    "        nike_df = pd.read_csv(os.path.join(path, file))\n",
    "        print(f\"Loaded {file} into DataFrame.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fdbe259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to only clothes\n",
    "nike_df = nike_df[nike_df[\"PRODUCT_TYPE\"] == \"APPAREL\"]\n",
    "# Pull item functions from last word of item\n",
    "nike_df.loc[:, \"Item function\"] = nike_df[\"TITLE\"].str.split().str[-1]\n",
    "nike_df = nike_df[nike_df[\"Item function\"] == \"Shirt\"]\n",
    "# nike_df.head()\n",
    "\n",
    "############## Relevant cols for final DF\n",
    "# # SKU, BRAND, Item function, PRICE_CURRENT, vibe (None!)\n",
    "##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d818e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\16155\\.cache\\kagglehub\\datasets\\joyshil0599\\h-and-m-sports-apparel-data-set9k\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"joyshil0599/h-and-m-sports-apparel-data-set9k\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "603b5de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Sports_H_and_M.csv into DataFrame.\n"
     ]
    }
   ],
   "source": [
    "# Load dataset file(s) into a pandas DataFrame\n",
    "# Assuming there's a CSV file in the folder\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith('.csv'):\n",
    "        hm_df = pd.read_csv(os.path.join(path, file))\n",
    "        print(f\"Loaded {file} into DataFrame.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "951d762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull item functions from last word of item\n",
    "hm_df.loc[:, \"Item function\"] = hm_df[\"Name_of_product\"].str.split().str[-1]\n",
    "# hm_df.head()\n",
    "\n",
    "############## Relevant cols for final DF\n",
    "# # SKU (None!), brand_name, Item function, price_of_product(in dollar), vibe (None!)\n",
    "##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce35fe5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\16155\\.cache\\kagglehub\\datasets\\whenamancodes\\adidas-us-retail-products-dataset\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"whenamancodes/adidas-us-retail-products-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2086d24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded adidas.csv into DataFrame.\n"
     ]
    }
   ],
   "source": [
    "# Load dataset file(s) into a pandas DataFrame\n",
    "# Assuming there's a CSV file in the folder\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith('.csv'):\n",
    "        adidas_df = pd.read_csv(os.path.join(path, file))\n",
    "        print(f\"Loaded {file} into DataFrame.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "600d4933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull item functions from last word of item\n",
    "adidas_df.loc[:, \"Item function\"] = adidas_df[\"name\"].str.split().str[-1]\n",
    "# adidas_df.head()\n",
    "\n",
    "############## Relevant cols for final DF\n",
    "# # sku, brand, Item function, selling_price, vibe (None!)\n",
    "##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dee472d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sku</th>\n",
       "      <th>brand</th>\n",
       "      <th>function</th>\n",
       "      <th>price</th>\n",
       "      <th>descriptive_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>H&amp;M</td>\n",
       "      <td>Shorts</td>\n",
       "      <td>12.99</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>H&amp;M</td>\n",
       "      <td>Joggers</td>\n",
       "      <td>39.99</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>H&amp;M</td>\n",
       "      <td>Joggers</td>\n",
       "      <td>39.99</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>H&amp;M</td>\n",
       "      <td>Hoodie</td>\n",
       "      <td>64.99</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>H&amp;M</td>\n",
       "      <td>Shorts</td>\n",
       "      <td>12.99</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sku brand function  price descriptive_tokens\n",
       "0  None   H&M   Shorts  12.99               None\n",
       "1  None   H&M  Joggers  39.99               None\n",
       "2  None   H&M  Joggers  39.99               None\n",
       "3  None   H&M   Hoodie  64.99               None\n",
       "4  None   H&M   Shorts  12.99               None"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Map columns in each DataFrame to a common column name\n",
    "nike_df.rename(columns={'SKU': 'sku', 'BRAND': 'brand', 'Item function': 'function', 'PRICE_CURRENT': 'price'}, inplace=True)\n",
    "hm_df.rename(columns={'brand_name': 'brand', 'Item function': 'function', 'price_of_product(in dollar)': 'price'}, inplace=True)\n",
    "adidas_df.rename(columns={'sku': 'sku', 'brand': 'brand', 'function': 'Item function', 'selling_price': 'price'}, inplace=True)\n",
    "\n",
    "# Step 2: Ensure each DataFrame has all the necessary columns ('sku', 'brand', 'Item function', 'price')\n",
    "columns_of_interest = ['sku', 'brand', 'function', 'price']\n",
    "\n",
    "# Manually add missing columns with None if they do not exist in the DataFrame\n",
    "for df in [nike_df, hm_df, adidas_df]:\n",
    "    for column in columns_of_interest:\n",
    "        if column not in df.columns:\n",
    "            df[column] = None\n",
    "\n",
    "# Step 3: Select only the relevant columns and concatenate the DataFrames\n",
    "df1 = nike_df[columns_of_interest]\n",
    "df2 = hm_df[columns_of_interest]\n",
    "df3 = adidas_df[columns_of_interest]\n",
    "\n",
    "apparel_data = pd.concat([df2, df3], ignore_index=True)\n",
    "\n",
    "# Step 4: Store the final DataFrame as a CSV with index\n",
    "apparel_data.to_csv('final_merged_data.csv', index=False)\n",
    "apparel_data['descriptive_tokens'] = None\n",
    "\n",
    "# Preview the final DataFrame\n",
    "apparel_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "915a98ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 4: Store the final DataFrame as a CSV with index\n",
    "# apparel_data.to_csv('apparel_data.csv', index=False)\n",
    "\n",
    "# # Preview the final DataFrame\n",
    "# print(apparel_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98589905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Shorts', 'Joggers', 'Hoodie', 'Jacket', 'Vest', 'Shirt', 'Pants',\n",
       "       'Parka', 'Windbreaker', 'Belt', 'Gaiters', 'DryMove', None],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apparel_data[\"function\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1711522",
   "metadata": {},
   "outputs": [],
   "source": [
    "combos = [['Jacket', 'Shirt', 'Pants'],\n",
    "          ['Jacket', 'Shirt', 'Shorts'],\n",
    "          ['Shirt', 'Joggers']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69adf469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba911e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use mini dataset\n",
    "feature_data = apparel_data[apparel_data[\"function\"].isin([\"Shorts\", \"Joggers\", \"Tank\", \"Hoodie\", \"Jacket\", \"Pants\", \"Shirt\"])]\n",
    "feature_data = feature_data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b22b7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000000/10000000 [01:14<00:00, 134856.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22749310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Precompute indices for each item in 'function' column\n",
    "function_to_indices = {func: feature_data[feature_data['function'] == func].index.tolist() \n",
    "                       for func in feature_data['function'].unique()}\n",
    "\n",
    "# Initialize the combo_counts hashmap (use defaultdict to simplify counting)\n",
    "combo_counts = defaultdict(int)\n",
    "\n",
    "# Iterate over the combos and generate the list of indexes\n",
    "num_repeats = 10000000\n",
    "for _ in tqdm(range(num_repeats)):\n",
    "    for combo in combos:\n",
    "        combo_indexes = []\n",
    "        for item in combo:\n",
    "            # Get the precomputed indices for the current item\n",
    "            indices = function_to_indices.get(item, [])\n",
    "            if indices:\n",
    "                selected_index = random.choice(indices)\n",
    "                combo_indexes.append(selected_index)\n",
    "            else:\n",
    "                combo_indexes.append(None)  # If no item is found, add None\n",
    "        \n",
    "        # Convert the combo indexes to a tuple and update the count\n",
    "        combo_tuple = tuple(combo_indexes)\n",
    "        combo_counts[combo_tuple] += 1\n",
    "\n",
    "# Output the hashmap with combo counts\n",
    "print(len(combo_counts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ecf996b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets = [x for x in combo_counts.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d9377551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import lil_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.sparse import coo_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "edee9ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Co-Occurrence Matrix: 100%|██████████| 22749310/22749310 [00:48<00:00, 466890.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-occurrence matrix shape: (6994, 6994)\n",
      "Non-zero entries: 125285048\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Initialize lists to hold the row indices, column indices, and values\n",
    "rows = []\n",
    "cols = []\n",
    "values = []\n",
    "\n",
    "# Step 2: Build the co-occurrence matrix from the \"buckets\" data\n",
    "for bucket in tqdm(buckets, desc=\"Building Co-Occurrence Matrix\"):\n",
    "    # Iterate over each pair of items in the bucket\n",
    "    for i in range(len(bucket)):\n",
    "        for j in range(i + 1, len(bucket)):\n",
    "            item_i, item_j = bucket[i], bucket[j]\n",
    "            # Append the indices and value (1 for co-occurrence) to the lists\n",
    "            rows.append(item_i)\n",
    "            cols.append(item_j)\n",
    "            values.append(1)  # Co-occurrence happens once for each pair\n",
    "            \n",
    "            rows.append(item_j)\n",
    "            cols.append(item_i)\n",
    "            values.append(1)  # Symmetric co-occurrence\n",
    "\n",
    "# Step 3: Create the sparse co-occurrence matrix using COO format\n",
    "num_items = len(feature_data)  # Number of items in the dataset\n",
    "co_occurrence_matrix = coo_matrix((values, (rows, cols)), shape=(num_items, num_items), dtype=np.float32)\n",
    "\n",
    "# Output the matrix shape and some statistics (optional)\n",
    "print(f\"Co-occurrence matrix shape: {co_occurrence_matrix.shape}\")\n",
    "print(f\"Non-zero entries: {co_occurrence_matrix.nnz}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a0d808ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6994"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32fa217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Build the co-occurrence matrix (could be sparse)\n",
    "# Assuming you already have a sparse matrix `co_occurrence_matrix`\n",
    "svd = TruncatedSVD(n_components=50)  # Reduce to 50 dimensions\n",
    "reduced_matrix = svd.fit_transform(co_occurrence_matrix)\n",
    "\n",
    "# Step 2: Apply K-means clustering in reduced space\n",
    "kmeans = KMeans(n_clusters=20)  # Set number of clusters\n",
    "kmeans.fit(reduced_matrix)\n",
    "\n",
    "# Step 3: Get the cluster labels\n",
    "labels = kmeans.labels_\n",
    "\n",
    "k_menas_labels = kmeans.fit_predict(reduced_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ddeaa9ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{np.int32(0),\n",
       " np.int32(1),\n",
       " np.int32(2),\n",
       " np.int32(3),\n",
       " np.int32(4),\n",
       " np.int32(6),\n",
       " np.int32(7),\n",
       " np.int32(8),\n",
       " np.int32(9),\n",
       " np.int32(10),\n",
       " np.int32(11),\n",
       " np.int32(12),\n",
       " np.int32(13),\n",
       " np.int32(14),\n",
       " np.int32(15),\n",
       " np.int32(16),\n",
       " np.int32(17),\n",
       " np.int32(18),\n",
       " np.int32(19)}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make an example closet\n",
    "closet = set()\n",
    "for i, (key, val) in enumerate(combo_counts.items()):\n",
    "    if i == 50: \n",
    "        break\n",
    "    \n",
    "    for item in key:\n",
    "        closet.add(k_menas_labels[item])\n",
    "    \n",
    "closet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ab1871d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of fits the current closet can make\n",
    "num_fits_init = 0\n",
    "init_fits = []\n",
    "for key, val in combo_counts.items():\n",
    "    if set(key).issubset(closet):\n",
    "        num_fits_init += 1\n",
    "        init_fits.append(key)\n",
    "\n",
    "print(num_fits_init)\n",
    "\n",
    "num_fits_new = 0\n",
    "closet_plus = closet.copy()\n",
    "closet_plus.add(40)\n",
    "for key, val in combo_counts.items():\n",
    "    if set(key).issubset(closet_plus):\n",
    "        num_fits_new += 1\n",
    "\n",
    "print(num_fits_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "df8aff85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Build inverted index: item → set of combo keys (tuples)\n",
    "item_to_combos = defaultdict(set)\n",
    "for combo in combo_counts:\n",
    "    for item in combo:\n",
    "        item_to_combos[item].add(combo)\n",
    "\n",
    "random_integers = random.sample(range(len(feature_data)), 500)\n",
    "samples_scores = []\n",
    "\n",
    "for r in random_integers:\n",
    "    curr_closet = closet.copy()\n",
    "    curr_closet.add(r)\n",
    "\n",
    "    # Step 2: Get candidate combos (those that share any item in closet)\n",
    "    candidate_combos = set()\n",
    "    for item in curr_closet:\n",
    "        candidate_combos.update(item_to_combos.get(item, []))\n",
    "    \n",
    "    # Step 3: Check only those combos\n",
    "    curr_score = 0\n",
    "    for combo in candidate_combos:\n",
    "        if set(combo).issubset(curr_closet):\n",
    "            curr_score += combo_counts[combo]\n",
    "    \n",
    "    samples_scores.append(curr_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "33715a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(82.8)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_percentile(data, quantity):\n",
    "    sorted_data = np.sort(data)  # Sort the data\n",
    "    num_values = len(sorted_data)\n",
    "    \n",
    "    # Count how many values are <= the quantity\n",
    "    count = np.sum(sorted_data <= quantity)\n",
    "    \n",
    "    # Calculate the percentile rank\n",
    "    percentile_rank = (count / num_values) * 100\n",
    "    \n",
    "    return percentile_rank\n",
    "\n",
    "calculate_percentile(samples_scores, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "007b80f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted array: [31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 27, 27, 27, 27, 27, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25]\n",
      "Original indices: [20, 28, 31, 465, 478, 466, 448, 454, 431, 456, 457, 432, 417, 44, 39, 35, 374, 407, 411, 45, 403, 401, 404, 405, 414, 346, 358, 365, 355, 60, 383, 381, 289, 294, 274, 66, 68, 70, 72, 341, 320, 321, 350, 351, 332, 65, 335, 333, 256, 285, 260, 279, 316, 219, 195, 198, 254, 225, 215, 93, 248, 270, 266, 265, 491, 175, 144, 165, 183, 178, 11, 494, 6, 1, 140, 179, 109, 97, 207, 203, 149, 115, 159, 126, 116, 154, 363, 376, 30, 425, 458, 329, 34, 410, 390, 397, 300, 147, 148, 172, 105, 92, 209, 194, 81, 268, 185, 124, 488, 482, 495, 499, 498, 496, 136, 138, 455, 453, 472, 450, 479, 121, 463, 461, 440, 442, 447, 421, 131, 132, 128, 408, 170, 42, 174, 145, 150, 152, 153, 444, 445, 446, 190, 161, 160, 168, 415, 412, 163, 37, 389, 384, 396, 402, 221, 192, 193, 196, 354, 58, 55, 54, 52, 102, 107, 98, 180, 101, 377, 372, 378, 379, 187, 189, 296, 295, 229, 78, 252, 250, 224, 348, 349, 236, 64, 330, 324, 74, 308, 366, 237, 212, 216, 214, 339, 50, 96, 202, 356, 359, 360, 16, 22, 25, 27, 18, 287, 286, 486, 485, 15, 480, 257, 259, 298, 83, 301, 302, 263, 269, 261, 262, 280, 278, 272, 275, 87, 84, 90, 282, 88, 240, 310, 243, 312, 317, 288, 292, 12, 17, 29, 169, 395, 113, 133, 41, 314, 91, 85, 276, 487, 71, 239, 326, 375, 393, 338, 199, 79, 117, 135, 40, 409, 369, 477, 3, 438, 151, 167, 191, 231, 232, 306, 345, 399, 181, 220, 340, 48, 471, 462, 459, 139, 95, 277, 497, 19, 21, 23, 24, 26, 464, 467, 468, 8, 9, 10, 13, 14, 481, 483, 484, 489, 490, 284, 258, 264, 267, 271, 80, 82, 297, 299, 303, 273, 281, 283, 86, 89, 94, 241, 242, 244, 245, 311, 313, 315, 318, 319, 290, 291, 293, 246, 247, 249, 251, 253, 255, 226, 227, 228, 230, 67, 69, 73, 75, 76, 77, 304, 305, 307, 309, 344, 347, 322, 323, 325, 327, 328, 331, 334, 233, 234, 235, 238, 208, 210, 211, 213, 217, 218, 357, 361, 362, 364, 367, 336, 337, 342, 343, 222, 223, 197, 200, 201, 204, 205, 206, 51, 53, 56, 57, 59, 61, 62, 63, 352, 353, 99, 100, 103, 104, 106, 108, 110, 111, 176, 177, 398, 368, 370, 371, 373, 380, 382, 49, 182, 184, 186, 188, 162, 164, 166, 413, 385, 386, 387, 388, 391, 392, 394, 430, 32, 33, 36, 38, 43, 46, 47, 400, 406, 171, 173, 146, 155, 156, 157, 158, 112, 416, 418, 419, 420, 422, 423, 424, 426, 427, 428, 429, 114, 118, 119, 120, 122, 123, 125, 127, 129, 130, 460, 433, 434, 435, 436, 437, 439, 441, 443, 469, 470, 473, 474, 475, 476, 449, 451, 452, 134, 137, 141, 142, 143, 2, 4, 5, 7, 492, 0, 493]\n"
     ]
    }
   ],
   "source": [
    "# Convert to NumPy array for sorting\n",
    "arr_np = np.array(samples_scores)\n",
    "\n",
    "# Get sorted indices by descending value\n",
    "sorted_indices = np.argsort(-arr_np)\n",
    "\n",
    "# Get sorted array using the indices\n",
    "sorted_arr = arr_np[sorted_indices]\n",
    "\n",
    "print(\"Sorted array:\", sorted_arr.tolist())\n",
    "print(\"Original indices:\", sorted_indices.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df17f5b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298eb63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find similar items -- Hard maybe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
